# Forensics Adapter - Model Architecture Explained

A visual guide to understanding the Forensics Adapter architecture and data flow.

---

## ğŸ—ï¸ High-Level Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     INPUT: Face Image                       â”‚
â”‚                      (256Ã—256 RGB)                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â”‚
                      â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                      â”‚                  â”‚
                      â–¼                  â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚   CLIP ViT-L/14     â”‚  â”‚   Adapter    â”‚
        â”‚   (FROZEN 304M)     â”‚  â”‚ (TRAIN 5.7M) â”‚
        â”‚                     â”‚  â”‚              â”‚
        â”‚ Extract features    â”‚  â”‚ Learn forgeryâ”‚
        â”‚ from layers:        â”‚  â”‚ boundaries   â”‚
        â”‚  â€¢ Layer 0          â”‚  â”‚              â”‚
        â”‚  â€¢ Layer 1          â”‚  â”‚ ViT-tiny     â”‚
        â”‚  â€¢ Layer 8          â”‚  â”‚ 128 queries  â”‚
        â”‚  â€¢ Layer 15         â”‚  â”‚              â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
                   â”‚                    â”‚
                   â”‚    Interaction     â”‚
                   â”‚    (Attention)     â”‚
                   â”‚                    â”‚
                   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
                   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                   â”‚   RecAttnClip      â”‚
                   â”‚  (Interaction)     â”‚
                   â”‚                    â”‚
                   â”‚ Combine CLIP +     â”‚
                   â”‚ Adapter features   â”‚
                   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
                   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                   â”‚  PostProcess       â”‚
                   â”‚  (Classification)  â”‚
                   â”‚                    â”‚
                   â”‚  MLP: 768â†’2        â”‚
                   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚                             â”‚
              â–¼                             â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  Classification  â”‚         â”‚  Forgery Maps    â”‚
    â”‚  [Real, Fake]    â”‚         â”‚  (Xray/Boundary) â”‚
    â”‚                  â”‚         â”‚                  â”‚
    â”‚  Softmaxâ†’Prob    â”‚         â”‚  256Ã—256         â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ”„ Data Flow

### 1. Input Stage
```
Original Image â†’ Face Detection â†’ Crop & Align â†’ Resize to 256Ã—256
                                                         â”‚
                                                         â”œâ†’ To CLIP (resize to 224)
                                                         â””â†’ To Adapter
```

### 2. Feature Extraction Stage
```
Image (224Ã—224)
    â”‚
    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         CLIP Visual Encoder                 â”‚
â”‚                                             â”‚
â”‚  Input: 224Ã—224Ã—3                           â”‚
â”‚                                             â”‚
â”‚  â†’ Patch Embedding (16Ã—16 patches)          â”‚
â”‚  â†’ 24 Transformer Layers                    â”‚
â”‚                                             â”‚
â”‚  Extract at specific layers:                â”‚
â”‚    Layer 0:  Initial features               â”‚
â”‚    Layer 1:  Low-level patterns             â”‚
â”‚    Layer 8:  Mid-level structures           â”‚
â”‚    Layer 15: High-level semantics           â”‚
â”‚                                             â”‚
â”‚  Output: 4 feature maps                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 3. Adapter Processing Stage
```
Image (256Ã—256) + CLIP Features
    â”‚
    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚            Adapter Network                  â”‚
â”‚                                             â”‚
â”‚  Architecture: ViT-tiny                     â”‚
â”‚  Learnable Queries: 128                     â”‚
â”‚                                             â”‚
â”‚  Process:                                   â”‚
â”‚  1. Extract visual features                 â”‚
â”‚  2. Apply cross-attention with queries      â”‚
â”‚  3. Learn forgery-specific patterns         â”‚
â”‚  4. Generate attention biases               â”‚
â”‚  5. Predict forgery boundaries (xray)       â”‚
â”‚                                             â”‚
â”‚  Focus: Blending boundaries where           â”‚
â”‚         fake faces are composited           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 4. Fusion Stage
```
CLIP Features + Adapter Attention Biases
    â”‚
    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚           RecAttnClip                       â”‚
â”‚                                             â”‚
â”‚  Mechanism: Attention-based interaction     â”‚
â”‚                                             â”‚
â”‚  Process:                                   â”‚
â”‚  1. Take CLIP's output features             â”‚
â”‚  2. Apply adapter's attention biases        â”‚
â”‚  3. Re-weight CLIP features                 â”‚
â”‚  4. Highlight forgery-relevant regions      â”‚
â”‚                                             â”‚
â”‚  Result: Enhanced feature representation    â”‚
â”‚          (general knowledge + forgery       â”‚
â”‚           specific knowledge)               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 5. Classification Stage
```
Enhanced Features
    â”‚
    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         PostClipProcess                     â”‚
â”‚                                             â”‚
â”‚  Architecture: MLP                          â”‚
â”‚  Input: 768-dim features                    â”‚
â”‚  Hidden: 256-dim                            â”‚
â”‚  Output: 2 classes [Real, Fake]             â”‚
â”‚                                             â”‚
â”‚  Process:                                   â”‚
â”‚  1. Linear projection                       â”‚
â”‚  2. ReLU activation                         â”‚
â”‚  3. Final classification layer              â”‚
â”‚  4. Softmax â†’ probabilities                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ¯ Training vs Inference

### Training Mode
```
Input Batch (B Ã— 3 Ã— 256 Ã— 256)
    â”‚
    â”œâ”€â”€â†’ CLIP (frozen) â†’ Features
    â”œâ”€â”€â†’ Adapter (train) â†’ Attention + Xray
    â””â”€â”€â†’ Interaction (train) â†’ Classification
           â”‚
           â”œâ”€â”€â†’ Classification Loss (CE)
           â”œâ”€â”€â†’ Xray Loss (MSE with GT mask)
           â”œâ”€â”€â†’ Intra-adapter Loss (consistency)
           â””â”€â”€â†’ CLIP Loss (feature alignment)
                  â”‚
                  â””â”€â”€â†’ Total Loss = 10Ã—L_cls + 200Ã—L_xray 
                                  + 20Ã—L_intra + 10Ã—L_clip
```

### Inference Mode
```
Input Image
    â”‚
    â”œâ”€â”€â†’ CLIP (frozen) â†’ Features
    â”œâ”€â”€â†’ Adapter (frozen) â†’ Attention + Xray
    â””â”€â”€â†’ Interaction (frozen) â†’ Classification
           â”‚
           â””â”€â”€â†’ Probability: P(Fake|Image)
                  â”‚
                  â”œâ”€â”€â†’ Single frame score
                  â””â”€â”€â†’ Video-level: Average of frame scores
```

---

## ğŸ“Š Parameter Distribution

```
Total Parameters: ~310M
â”œâ”€ CLIP ViT-L/14:     ~304M (FROZEN â„ï¸)
â”‚  â””â”€ Not updated during training
â”‚
â””â”€ Trainable:         ~5.7M (TRAINED ğŸ”¥)
   â”œâ”€ Adapter:        ~5.0M
   â”‚  â”œâ”€ ViT-tiny backbone
   â”‚  â”œâ”€ 128 learnable queries
   â”‚  â””â”€ Cross-attention layers
   â”‚
   â”œâ”€ RecAttnClip:    ~0.5M
   â”‚  â””â”€ Attention interaction
   â”‚
   â””â”€ PostProcess:    ~0.2M
      â””â”€ Classification head
```

**Key Insight**: Only 1.8% of parameters are trainable!
- Prevents overfitting
- Retains CLIP's generalization
- Adds task-specific knowledge

---

## ğŸ§  What Each Component Learns

### CLIP (Frozen)
**Provides**: General visual understanding
- Object recognition
- Scene understanding
- Semantic features
- Pretrained on 400M image-text pairs

**Why Frozen?**
- Already has strong generalization
- Training on limited deepfake data would overfit
- Serves as stable feature extractor

### Adapter (Trainable)
**Learns**: Forgery-specific patterns
- Blending boundaries
- Inconsistent lighting
- Unnatural textures
- Face-background transitions

**Why Small?**
- Prevents overfitting on training data
- Forces learning of generalizable patterns
- Doesn't override CLIP's knowledge

### RecAttnClip (Trainable)
**Learns**: How to combine knowledge
- Which CLIP features are relevant
- How to weight adapter's findings
- Where to focus attention
- Feature fusion strategy

### PostProcess (Trainable)
**Learns**: Final decision mapping
- How to interpret combined features
- Classification boundary
- Confidence calibration

---

## ğŸ” Why This Architecture Works

### 1. Leverages Pre-trained Knowledge
```
CLIP (trained on 400M images)
    â””â”€â”€â†’ General visual understanding
         â””â”€â”€â†’ Transfers to face images
              â””â”€â”€â†’ Provides strong baseline
```

### 2. Adds Task-Specific Learning
```
Adapter (trained on deepfakes)
    â””â”€â”€â†’ Learns forgery patterns
         â””â”€â”€â†’ Complements CLIP
              â””â”€â”€â†’ Improves detection
```

### 3. Efficient Interaction
```
Attention Mechanism
    â””â”€â”€â†’ Combines general + specific
         â””â”€â”€â†’ Highlights relevant features
              â””â”€â”€â†’ Robust classification
```

### 4. Strong Generalization
```
Small trainable part (5.7M)
    â””â”€â”€â†’ Avoids overfitting
         â””â”€â”€â†’ Generalizes to new datasets
              â””â”€â”€â†’ Cross-dataset performance
```

---

## ğŸ¨ Forgery Detection Process

### Step-by-Step:

**1. Input Image**
```
Face image with potential forgery
```

**2. CLIP Processing**
```
Extract multi-level features:
- Low-level: edges, textures
- Mid-level: face parts, structures
- High-level: identity, expression
```

**3. Adapter Analysis**
```
Focus on forgery traces:
- Find blending boundaries
- Detect inconsistencies
- Generate attention maps
```

**4. Feature Fusion**
```
Combine CLIP + Adapter:
- Enhance forgery-relevant features
- Suppress irrelevant information
- Create discriminative representation
```

**5. Classification**
```
Map features to probability:
- P(Real) = low â†’ likely real
- P(Fake) = high â†’ likely fake
- Output confidence score
```

**6. Video Aggregation** (if applicable)
```
For video:
- Collect frame predictions
- Average scores
- More stable than single frame
```

---

## ğŸ“ˆ Information Flow Visualization

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Input Image  â”‚  Raw facial image
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ CLIP Branch  â”‚  General features (what is it?)
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
       â”‚             â”‚
       â–¼             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Adapter      â”‚ â”‚ CLIP Output  â”‚
â”‚ Branch       â”‚ â”‚              â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚                â”‚
       â”‚ Forgery maps   â”‚ Visual features
       â”‚ Attention bias â”‚
       â”‚                â”‚
       â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
                â”‚
                â–¼
       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
       â”‚ Feature Fusion â”‚  Enhanced representation
       â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
                â”‚
                â–¼
       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
       â”‚ Classification â”‚  Real or Fake?
       â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
                â”‚
                â–¼
       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
       â”‚ Probability    â”‚  Confidence score
       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ”¬ Key Innovations

### 1. Adapter Design
- **Small**: Only 5.7M parameters
- **Focused**: Learns specific forgery patterns
- **Efficient**: Doesn't require retraining CLIP

### 2. Fusion Strategy
- **Attention-based**: Dynamic feature weighting
- **Layer-specific**: Fuses at multiple CLIP layers
- **Bidirectional**: Information flows both ways

### 3. Multi-Task Learning
- **Classification**: Real vs Fake
- **Boundary Detection**: Xray prediction
- **Feature Consistency**: Intra-adapter loss
- **CLIP Alignment**: Maintains feature quality

### 4. Generalization Mechanism
- **CLIP's versatility**: Trained on diverse data
- **Adapter's specificity**: Task-focused learning
- **Small parameter count**: Prevents overfitting
- **Multi-dataset training**: Improves robustness

---

## ğŸ¯ Comparison with Other Approaches

### Traditional CNN-based:
```
Input â†’ CNN â†’ Classification
Problems: Overfits to training data, poor generalization
```

### Fine-tuned CLIP:
```
Input â†’ CLIP (fine-tuned) â†’ Classification
Problems: Loses general knowledge, requires many parameters
```

### Forensics Adapter (This Work):
```
Input â†’ CLIP (frozen) + Adapter (small) â†’ Classification
Benefits: Retains generalization, efficient, task-specific
```

---

## ğŸ’¡ Why Your Results Make Sense

### Frame-Level Accuracy: 59%
```
Individual frames â†’ Noisy predictions
Some frames lack clear forgery signs
Model must classify every frame
â†’ Moderate accuracy expected
```

### Frame-Level AUC: 78%
```
Across all thresholds â†’ Good separation
Model distinguishes real/fake patterns
Reasonable discrimination ability
â†’ Good performance
```

### Video-Level AUC: 100%
```
Averaged per video â†’ Smooth predictions
Noise cancels out
Clear video-level patterns
â†’ Perfect separation
```

**Conclusion**: Architecture designed for video-level performance!

---

## ğŸ“ Understanding the "Adapter" Concept

### What is an Adapter?
```
Pretrained Model (Frozen)
        â”‚
        â”œâ”€â”€â†’ Small trainable module (Adapter)
        â”‚         â”‚
        â”‚         â””â”€â”€â†’ Task-specific knowledge
        â”‚
        â””â”€â”€â†’ Combined output
```

### Why Adapters Work:
1. **Preserve** pre-trained knowledge
2. **Add** task-specific capabilities
3. **Efficient** (few parameters)
4. **Flexible** (easy to swap)

### Applied to Deepfake Detection:
- **Preserve**: CLIP's visual understanding
- **Add**: Forgery detection capability
- **Result**: Best of both worlds!

---

## ğŸš€ Practical Implications

### For Research:
- Efficient fine-tuning approach
- Strong baseline for future work
- Interpretable (attention maps)
- Extensible architecture

### For Deployment:
- Reliable video-level detection
- Generalizes across datasets
- Reasonable computational cost
- Clear decision boundaries

### For Understanding Deepfakes:
- Shows importance of blending boundaries
- Validates multi-level feature approach
- Demonstrates value of pre-training
- Highlights generalization challenges

---

**Summary**: The Forensics Adapter cleverly combines CLIP's general visual knowledge with a small, specialized adapter network to achieve strong, generalizable deepfake detection. Your results confirm this design works as intended!

---

*Model Architecture Guide for Forensics Adapter*  
*Last Updated: November 18, 2025*

